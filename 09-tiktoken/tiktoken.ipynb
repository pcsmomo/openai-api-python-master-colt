{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72318c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43573b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c126c3d",
   "metadata": {},
   "source": [
    "## Counting Tokens In Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3517c238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc.encode(\"Hello world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc3c1938",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_text = \"\"\"\n",
    "Mrs. Darling quivered and went to the window. It was securely fastened.\n",
    "She looked out, and the night was peppered with stars. They were\n",
    "crowding round the house, as if curious to see what was to take place\n",
    "there, but she did not notice this, nor that one or two of the smaller\n",
    "ones winked at her. Yet a nameless fear clutched at her heart and made\n",
    "her cry, “Oh, how I wish that I wasn’t going to a party to-night!”\n",
    "\n",
    "Even Michael, already half asleep, knew that she was perturbed, and he\n",
    "asked, “Can anything harm us, mother, after the night-lights are lit?”\n",
    "\n",
    "“Nothing, precious,” she said; “they are the eyes a mother leaves\n",
    "behind her to guard her children.”\n",
    "\n",
    "She went from bed to bed singing enchantments over them, and little\n",
    "Michael flung his arms round her. “Mother,” he cried, “I’m glad of\n",
    "you.” They were the last words she was to hear from him for a long\n",
    "time.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cac4f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc.encode(book_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e399f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, model_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d246a5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(book_text, \"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d492bf80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(book_text, \"text-davinci-003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f61760d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_per_token = 0.002 / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "832d4cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000448"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(book_text, \"gpt-3.5-turbo\") * price_per_token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30b8c047",
   "metadata": {},
   "source": [
    "## Counting Tokens in Messages (for Chat API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2beffc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model == \"gpt-3.5-turbo\":\n",
    "        print(\"Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")\n",
    "    elif model == \"gpt-4\":\n",
    "        print(\"Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0314\")\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif model == \"gpt-4-0314\":\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b74d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"New synergies will help drive top-line growth.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Things working well together will increase revenue.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d8b1589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_messages(example_messages, \"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c4e26b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_messages(example_messages, \"gpt-4\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e948f02d",
   "metadata": {},
   "source": [
    "### Confirming Token Counts With a Real Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0f425f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import dotenv_values\n",
    "config = dotenv_values(\"../.env\")\n",
    "openai.api_key = config[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c72f4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7Dhk079Eg9ql77LUTU6NcE0eeXdfD at 0x10a126210> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"This\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1683500760,\n",
       "  \"id\": \"chatcmpl-7Dhk079Eg9ql77LUTU6NcE0eeXdfD\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 1,\n",
       "    \"prompt_tokens\": 127,\n",
       "    \"total_tokens\": 128\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=example_messages,\n",
    "    max_tokens=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
